{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c80aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## My API key  & library_Id are stored in another file\n",
    "api_key,library_id = pd.read_json(\"Zotero_API_key.json\", typ='series')\n",
    "\n",
    "library_type=\"group\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyzotero import zotero\n",
    "zot = zotero.Zotero(library_id, library_type, api_key)\n",
    "\n",
    "ID_Universities='EWSYI3RS' # to access the files in the university subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff5a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in zot.collections_sub(ID_Universities):\n",
    "    name,key=i[\"data\"][\"name\"],i[\"data\"][\"key\"]\n",
    "    print(f\"name: {name} --> key: {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f16c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevantItemInfo(dictionary):    \n",
    "    if dictionary[\"itemType\"]=='attachment':\n",
    "        # if directory --> key leads to the correct folder\n",
    "        \n",
    "        relevant_keys=['key','url']\n",
    "        newDict={key: dictionary[key] for key in relevant_keys}\n",
    "        #print(dictionary)\n",
    "        newDict[\"note\"]=None\n",
    "       # print(newDict)\n",
    "        return newDict\n",
    "    \n",
    "    if dictionary[\"itemType\"]==\"note\":\n",
    "        relevant_keys=['key','note']\n",
    "        newDict={key: dictionary[key] for key in relevant_keys}\n",
    "        newDict[\"url\"]=None\n",
    "        return newDict\n",
    "\n",
    "    \n",
    "\n",
    "for i in zot.collection_items(key)[:2]:\n",
    "    dictionary=i[\"data\"]\n",
    "    print(dictionary, end=\"\\n\\n\")\n",
    "    \n",
    "    Info=relevantItemInfo(dictionary)\n",
    "    print(Info, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c774117",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictList=[]\n",
    "\n",
    "for i in zot.collections_sub(ID_Universities):\n",
    "    name,key=i[\"data\"][\"name\"],i[\"data\"][\"key\"]\n",
    "    print(f\"name: {name} --> key: {key}\")\n",
    "    \n",
    "    for i in zot.collection_items(key):\n",
    "        # only use data if there is an attachment.\n",
    "        if i[\"data\"][\"itemType\"]=='attachment':\n",
    "            Info=relevantItemInfo(i[\"data\"])\n",
    "            dictList+=[{\"Uni\": name,\"FileKey\":Info[\"key\"],\"url\":Info[\"url\"]}]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc36e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(dictList)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2234f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FileLinks(Filekey):\n",
    "   #print(Filekey)\n",
    "    import os\n",
    "    DIR=f\"C:\\\\Users\\\\mr\\\\Zotero\\\\storage\\\\{Filekey}\"\n",
    "    #print(DIR)\n",
    "    FileNames=os.listdir(DIR)\n",
    "    file=[i for i in FileNames if i.endswith(\".html\")]\n",
    "    return DIR+\"\\\\\"+FileNames[-1]\n",
    "\n",
    "\n",
    "\n",
    "df[\"file\"]=df.FileKey.apply(FileLinks)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96e1c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"2023-05-10_Uni_Files_raw.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f8a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(\"2023-05-10_Uni_Files_raw.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f98c63",
   "metadata": {},
   "source": [
    "# Add Content from HTMLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b8939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from htmldate import find_date\n",
    "#find_date(\"https://www.uu.nl/en/events/chat-gpt-discover-the-magic-of-ai\")\n",
    "\n",
    "def find_dateX(x):\n",
    "    try:\n",
    "        return find_date(x)\n",
    "    except:\n",
    "        print(x)\n",
    "\n",
    "\n",
    "df[\"date\"]=df.url.apply(find_dateX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb75526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MyLib.nlp as nlp\n",
    "import MyLib.HTML_prep as HTML_prep\n",
    "import MyLib.analysis as analysis \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949a4b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def FileInfo(FilePath):\n",
    "    Title,h1,h2,text,linkName,linkUrl=None,None,None,None,None,None\n",
    "    if FilePath.endswith(\".html\"):\n",
    "    \n",
    "        try:\n",
    "            #print(FilePath)\n",
    "\n",
    "            with open(FilePath,\"r\", encoding='utf-8') as f:\n",
    "                text= f.read()\n",
    "\n",
    "            from bs4 import BeautifulSoup\n",
    "\n",
    "            soup = BeautifulSoup(text, 'html.parser')\n",
    "            \n",
    "            Title=soup.title.text\n",
    "            \n",
    "            #soup=HTML_prep.removeIMGs(soup,IMG_filename=Title.replace(\" \",\"_\"))      \n",
    "\n",
    "\n",
    "            All_divs=[p.parent for p in soup(\"p\") if p.parent.name==\"div\"]\n",
    "            All_divs=(list(set(All_divs)))\n",
    "            All_divs=[HTML_prep.removeIMGs(div,no_img=True,IMG_filename=Title.replace(\" \",\"_\")) for div in All_divs]        \n",
    "\n",
    "            h1=[item for sublist in [[i.text for i in div(\"h1\")] for div in All_divs if div(\"h1\")!=None] for item in sublist]\n",
    "            h2=[item for sublist in [[i.text for i in div(\"h2\")] for div in All_divs if div(\"h2\")!=None] for item in sublist]\n",
    "            text=[item for sublist in [[i.text for i in div(\"p\")] for div in All_divs if div(\"p\")!=None] for item in sublist]\n",
    "            linkName=[item for sublist in [[i.text for i in div(\"a\")] for div in All_divs if div(\"a\")!=None] for item in sublist]\n",
    "            linkUrl=[item for sublist in [[i.get('href') for i in div(\"a\")] for div in All_divs if div(\"a\")!=None] for item in sublist]\n",
    "\n",
    "\n",
    "        except:\n",
    "            print(f\"error with: {FilePath}\",end=\". \")\n",
    "    return pd.Series([Title,h1,h2,text,linkName,linkUrl])\n",
    "        \n",
    "        \n",
    "\n",
    "#df.file[2:6].apply(FileInfo)\n",
    "df[[\"Title\",\"h1\",\"h2\",\"text\",\"linkName\",\"linkUrl\"]]=df.file.apply(FileInfo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c99ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"2023-05-10_Uni_Files_NLP.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "586e1b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(\"2023-05-10_Uni_Files_NLP.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b75fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Insert_title(text,Title):\n",
    "    if Title!=None and type(text)==list:\n",
    "        text=[Title]+text\n",
    "        return text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "\n",
    "df[\"text\"]=df[[\"text\",\"Title\"]].apply(lambda x: Insert_title(*x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b0702aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatGPT_Terms=\"ChatGPT, GPT3, GPT4, OpenAI, AI, LLM, A.I, GPT, Chatbot, technology, tool, app, Artificial Intelligence, Large Language Models\"\n",
    "\n",
    "df[\"AI_paragraphs\"]=df.text.apply(nlp.filter_paragraphs,by=ChatGPT_Terms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f46d097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "column=\"AI_paragraphs\"\n",
    "\n",
    "df=df[df.file.apply(lambda x: x.endswith(\"html\"))]\n",
    "\n",
    "df=df[df.Title!='Research Posters - Faculty of Geosciences - Utrecht University']\n",
    "\n",
    "df=df.explode(column).reset_index(drop=True)\n",
    "df=df[df[column].apply(lambda x: type(x)==str)]\n",
    "df=df.drop_duplicates(column)\n",
    "df[\"LEN\"]=df[column].apply(lambda x: len(x.split(\" \")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "95cf2d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning done.\n",
      "language detection done.\n",
      "Translating...\n",
      "nl Aan de han\n",
      "nl Sinds de r\n",
      "nl Tijdens de\n",
      "nl Jivet wijs\n",
      "nl In de work\n",
      "nl Let op: he\n",
      "nl ChatGPT ke\n",
      "nl Deze en no\n",
      "nl Ga het ges\n",
      "nl Sander Bax\n",
      "nl ChatGPT sc\n",
      "nl ChatGPT ka\n",
      "nl In een wet\n",
      "nl De komst v\n",
      "nl Het HOP in\n",
      "nl ChatGPT ha\n",
      "nl In het Bri\n",
      "nl Een van de\n",
      "nl Eerder mel\n",
      "nl ChatGPT ka\n",
      "nl De nieuwe \n",
      "nl Ook op Til\n",
      "nl Maar we zi\n",
      "nl We vroegen\n",
      "nl Daarop wer\n",
      "nl AI gaat im\n",
      "nl De zes uni\n",
      "nl “De miljoe\n",
      "nl Nuijten he\n",
      "nl Wie al een\n",
      "nl “We voldoe\n",
      "nl De TU/e is\n",
      "nl De start v\n",
      "nl UM zoekend\n",
      "nl Nieuwe mog\n",
      "nl ChatGPT on\n",
      "nl \"Voor de b\n",
      "nl De geavanc\n",
      "nl Die heeft \n",
      "nl Volgens ha\n",
      "nl Het is al \n",
      "nl En hoe ga \n",
      "nl Dat maakt \n",
      "nl Universita\n",
      "nl Google pub\n",
      "nl Op dinsdag\n",
      "nl De hype ro\n",
      "nl Hij is hee\n",
      "nl Wat ook he\n",
      "nl In een con\n",
      "nl \"We moeten\n",
      "nl Kijk eens \n",
      "nl Universite\n",
      "nl Het Univer\n",
      "nl In haar on\n",
      "nl In een eer\n",
      "nl ‘Een prett\n",
      "nl Ischen ond\n",
      "nl Onderzoek \n",
      "nl Als online\n",
      "nl Kunstmatig\n",
      "nl Chatbot ho\n",
      "nl Kun je ver\n",
      "nl Sporters w\n",
      "nl Een voetba\n",
      "nl Onze mense\n",
      "nl Docenten b\n",
      "nl Moet ChatG\n",
      "nl ‘Tekst- en\n",
      "nl Aan de Ned\n",
      "nl Axel Arnba\n",
      "nl ChatGPT in\n",
      "nl ‘Dit artik\n",
      "nl De gevolge\n",
      "nl ‘Met elkaa\n",
      "nl Van Dis, B\n",
      "nl Een chatbo\n",
      "nl Op 16 janu\n",
      "nl Er zijn ec\n",
      "nl Jelle Zuid\n",
      "nl Maar hoe b\n",
      "nl Maar als h\n",
      "nl Vergeet ni\n",
      "nl Let op, al\n",
      "nl Voor meer \n",
      "nl Hoe ga je \n",
      "nl Tips over \n",
      "nl AI content\n",
      "nl Engelstali\n",
      "nl De deelnem\n",
      "nl Slides Cha\n",
      "nl Dit is nie\n",
      "nl Verschille\n",
      "nl Verzamelin\n",
      "pure english text done. Next: Token & Lemmatizing.\n",
      "Token & Lemmatizing done. Next: Remove Stopwords.\n"
     ]
    }
   ],
   "source": [
    "df=nlp.NLP_Pipeline(df, sentiment=False, language=\"en\",translate=True, column=column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a562e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.text=df2[\"AI_paragraphs\"].apply(lambda x:\" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495a14cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2.text.apply(lambda x: type(x))!=str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca323ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"LEN\"]=df2.text.apply(lambda x: len(x))\n",
    "df2=df2[df2.LEN<df2.LEN.max()] ###Cut out the very large list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea308524",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.LEN.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "feb10742",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "15dc5a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"2023-05-15_Zotero_AI_nlp_en.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983e0050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3bebd67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf3f6b161134014a4a8339cd39d3bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='column', options=('NoStopwords',), value='NoStopwords'), IntSlider…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interactive, interact\n",
    "\n",
    "@interact(column=[\"NoStopwords\"],Mostcommon=(5,200))\n",
    "def make_wordcloud(column=\"NoStopwords\",Mostcommon=20,Save_as=\"\"):\n",
    "    hashtags=[i.lower() for s in df[column].dropna() for i in s] # hashtags OR # no stopwords\n",
    "    analysis.make_wordcloud(hashtags,filename=Save_as,Mostcommon=Mostcommon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dd0982",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_excel(\"2023-05-08_ZoteroHTMLs.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
