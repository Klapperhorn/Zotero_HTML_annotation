{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import MyLib.nlp as nlp\n",
    "import MyLib.HTML_prep as HTML_prep\n",
    "import MyLib.analysis as analysis \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "## My API key  & library_Id are stored in another file\n",
    "api_key,library_id = pd.read_json(\"Zotero_API_key.json\", typ='series')\n",
    "\n",
    "library_type=\"group\"\n",
    "\n",
    "def print_time():\n",
    "    from datetime import datetime\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Current Time =\", current_time)\n",
    "\n",
    "\n",
    "from pyzotero import zotero\n",
    "zot = zotero.Zotero(library_id, library_type, api_key)\n",
    "\n",
    "ID_Universities='EWSYI3RS' # to access the files in the university subfolder.\n",
    "ID_SURF='WPXRQVIU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c774117",
   "metadata": {},
   "outputs": [],
   "source": [
    "helpDict={}\n",
    "\n",
    "for i in zot.collections_sub(ID_Universities):\n",
    "    name,key=i[\"data\"][\"name\"],i[\"data\"][\"key\"]\n",
    "    print(f\"name: {name} --> key: {key}\")\n",
    "    collection_items=zot.collection_items(key)\n",
    "    helpDict.update({k[\"key\"]:k[\"data\"]|{\"Uni\":name} for k in collection_items})\n",
    "    \n",
    "#Add Surf\n",
    "name=\"surf\"\n",
    "key=ID_SURF\n",
    "print(f\"name: {name} --> key: {key}\")\n",
    "collection_items=zot.collection_items(key)\n",
    "helpDict.update({k[\"key\"]:k[\"data\"]|{\"Uni\":name} for k in collection_items})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480d364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(helpDict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b7f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Claimed_Parents=df[\"parentItem\"].dropna().to_list()\n",
    "Real_Parents=df.index.to_list()\n",
    "WithoutParents=list(set(Claimed_Parents)-set(Real_Parents))\n",
    "WithoutParents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2855183",
   "metadata": {},
   "outputs": [],
   "source": [
    "Parents_columns=['title', 'date','itemType','language']\n",
    "WithParents_columns=[\"key\",'parentItem','url','filename',\"Uni\"]\n",
    "\n",
    "WithParents=df.dropna(subset=\"parentItem\")[WithParents_columns]\n",
    "Parents=df[df.itemType.apply(lambda x: x!=\"attachment\")][Parents_columns]\n",
    "\n",
    "df=WithParents.merge(Parents, left_on=\"parentItem\",right_index=True, how=\"right\")\n",
    "df.head(5)\n",
    "\n",
    "#df2[df2.filename.isna()].itemType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec46b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Uni.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df35375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct item type of uTwente Thesies\n",
    "df.loc[df.url.apply(lambda x: \"essay.utwente.nl\" in str(x)),\"itemType\"]=\"thesis\"\n",
    "print(df.itemType.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d0d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove not useful items\n",
    "DropTypeList=[\"journalArticle\",\"conferencePaper\",\"thesis\",\"book\",\"bookSection\",\"note\",\"film\",\"dataset\"]\n",
    "\n",
    "df=df[~df[\"itemType\"].isin(DropTypeList)]\n",
    "\n",
    "print(df.itemType.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012ea62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileName(key,filename):\n",
    "    DIR=f\"C:\\\\Users\\\\mr\\\\Zotero\\\\storage\\\\{key}\\\\\"\n",
    "    filename=DIR+str(filename)\n",
    "    return filename\n",
    "\n",
    "df[\"filepath\"]=df.apply(lambda x: fileName(x.key, x.filename), axis=1).drop_duplicates()\n",
    "\n",
    "print(df.filepath[0],print(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ae84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"2023-07-30_Uni_Files_raw.json\")\n",
    "# CHeck for duplicates / errors in Zotero: \n",
    "df[df.index.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99b692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(\"2023-07-27_Uni_Files_raw.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb7a3f",
   "metadata": {},
   "source": [
    "# Add Content from PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f8a5e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getLinksfromPDF(page):\n",
    "    links=[]\n",
    "\n",
    "    if '/Annots' in page.keys():\n",
    "        annotations=page['/Annots']\n",
    "        for a in annotations:\n",
    "            a=a.get_object()\n",
    "            if '/A' in a:\n",
    "                A=a['/A']\n",
    "                if '/URI' in A.keys():\n",
    "                    link=(A['/URI'])\n",
    "                    links.append(link)\n",
    "    return links\n",
    "\n",
    "def getDate(pdf_reader):\n",
    "    mod_date=None\n",
    "    metaData=pdf_reader.metadata\n",
    "    if '/ModDate' in metaData.keys():\n",
    "        #print(metaData.keys())\n",
    "        mod_date=metaData['/ModDate'][2:10]\n",
    "        dtformat = \"%Y%m%d\"\n",
    "        mod_date=pd.to_datetime(mod_date,format=dtformat)\n",
    "        #print(creation_date)\n",
    "    return mod_date\n",
    "    \n",
    "def extract_text_with_pyPDF(filepath,MaxPages=20):\n",
    "    \n",
    "    pages,links,mod_date=[],[],None\n",
    "    from pypdf import PdfReader\n",
    "    \n",
    "    pdf_reader = PdfReader(filepath)\n",
    "    mod_date=getDate(pdf_reader)\n",
    "    \n",
    "    S_pages=pdf_reader.pages\n",
    "    if len(S_pages)>MaxPages:\n",
    "        f=filepath.split(\"\\\\\")[-1]\n",
    "        #print(f\"{f} has more than {MaxPages} pages: {len(S_pages)}. Only processing {MaxPages} pages.\")\n",
    "        S_pages=S_pages[:MaxPages]\n",
    "\n",
    "    for i, page in enumerate(S_pages):\n",
    "        raw_text = \"\"\n",
    "        try:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                raw_text += text\n",
    "                pages.append(raw_text)\n",
    "        except Exception as error:\n",
    "            print(\"text-problems with: \", filepath)\n",
    "            print(\"\\n\", error)\n",
    "            \n",
    "    for i, page in enumerate(S_pages):\n",
    "        links=getLinksfromPDF(page)\n",
    "\n",
    "    return pd.Series([pages,links,mod_date])\n",
    "\n",
    "    \n",
    "PDFs=df.loc[df.filepath.apply(lambda x: x.split(\".\")[-1]==\"pdf\"),[\"filepath\",\"date\"]]    \n",
    "NumberOfPDFs=len(PDFs)    \n",
    "\n",
    "print(f\"the dataset containts {NumberOfPDFs} PDF-files.\")\n",
    "\n",
    "df[[\"text\",\"links\",\"mod_date\"]]=PDFs.filepath.apply(extract_text_with_pyPDF,MaxPages=150)\n",
    "\n",
    "# Use filemod as date.\n",
    "df.loc[~df.mod_date.isna(),[\"date\"]]=df.mod_date\n",
    "df.drop(columns='mod_date',inplace=True)\n",
    "\n",
    "#problemfile.filepath.apply(extract_text_with_pyPDF,MaxPages=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f98c63",
   "metadata": {},
   "source": [
    "# Add Content from HTMLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8dee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_date(url, date):\n",
    "    from htmldate import find_date\n",
    "    from numpy import nan\n",
    "    if type(date)!=pd.Timestamp:\n",
    "        try:\n",
    "            date=pd.to_datetime(find_date(url))\n",
    "        except:\n",
    "            date=nan\n",
    "    return date\n",
    "\n",
    "\n",
    "print(\"This takes about: 3 minutes.\")\n",
    "\n",
    "df[\"date\"]=df.date.apply(pd.to_datetime,errors='coerce')\n",
    "print_time()     \n",
    "\n",
    "df[\"date\"]=df.apply(lambda x: find_date(x.url, x.date), axis=1)\n",
    "print_time()\n",
    "\n",
    "df.date.apply(lambda x: type(x)==pd.Timestamp).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4f2046",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"2023-07-30_Uni_Files_raw_date.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d44a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(\"2023-07-30_Uni_Files_raw_date.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5ddb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949a4b81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def FileInfo(FilePath):\n",
    "    Title,h1,h2,h3,text,linkName,linkUrl=None,None,None,None,None,None,None\n",
    "\n",
    "    if FilePath.endswith(\".html\"):\n",
    "           \n",
    "        try:\n",
    "            #print(FilePath)\n",
    "\n",
    "            with open(FilePath,\"r\", encoding='utf-8') as f:\n",
    "                text= f.read()\n",
    "            \n",
    "        except:\n",
    "            print(\"error opening the html file. File does not exist?\")\n",
    "            return\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            from bs4 import BeautifulSoup\n",
    "\n",
    "            soup = BeautifulSoup(text, \"html5lib\") #'html.parser')       \n",
    "            Title=soup.title.text\n",
    "            #soup=HTML_prep.removeIMGs(soup,IMG_filename=Title.replace(\" \",\"_\"))\n",
    "            \n",
    "            TextIndicators=soup(\"p\")+soup(\"h5\")+soup(\"h4\")+soup(\"h3\")+soup(\"h2\")+soup(\"h1\")\n",
    "\n",
    "            All_divs=[p.parent for p in TextIndicators if p.parent.name in [\"div\",\"main\",\"section\",\"article\",\"center\",\"td\"]]\n",
    "     \n",
    "            All_divs=list(dict.fromkeys(All_divs))\n",
    "            \n",
    "            \n",
    "            # Remove images\n",
    "            try:\n",
    "                IMG_filename=\"\".join(x for x in Title.strip().replace(\" \",\"_\") if x.isalnum() or x==\"_\")[:100]\n",
    "                All_divs=[HTML_prep.removeIMGs(div,write_img=False,IMG_filename=IMG_filename) for div in All_divs]        \n",
    "            except:\n",
    "                print(f\"error removing images: {FilePath}\")\n",
    "                \n",
    "            h1=[item for sublist in [[i.text for i in div(\"h1\")] for div in All_divs if div(\"h1\")!=None] for item in sublist]\n",
    "            h2=[item for sublist in [[i.text for i in div(\"h2\")] for div in All_divs if div(\"h2\")!=None] for item in sublist]\n",
    "            h3=[item for sublist in [[i.text for i in div(\"h3\")] for div in All_divs if div(\"h3\")!=None] for item in sublist]\n",
    "        \n",
    "\n",
    "            # here only get all divs\n",
    "            #text=[item for sublist in [[i.text for i in div(\"p\")] for div in All_divs if div(\"p\")!=None] for item in sublist]\n",
    "            \n",
    "            # here gets all human readable text parts --> includes headlines. the \\n \\n keeps distance to headlines\n",
    "           \n",
    "            linkName=[item for sublist in [[i.text for i in div(\"a\")] for div in All_divs if div(\"a\")!=None] for item in sublist]\n",
    "            linkUrl=[item for sublist in [[i.get('href') for i in div(\"a\")] for div in All_divs if div(\"a\")!=None] for item in sublist]\n",
    "            #i=i(\"p\")+i(\"h5\")+i(\"h4\")+i(\"h3\")+i(\"h2\")+i(\"h1\")\n",
    "            text=[i.get_text(separator=u' ').replace(\".\",\". \").replace(\"\\n \",\". \") for i in All_divs if i.get_text()!=None]\n",
    "\n",
    "        except:\n",
    "            print(f\"error with: {FilePath}\", end=\". \")\n",
    "            \n",
    "            try:\n",
    "                text=text.split(\"\\n\\n\")\n",
    " \n",
    "                print(\"--> text from reading as a text file.\")\n",
    "            except:\n",
    "                print(\"also no text file\")\n",
    "                \n",
    "        #leave out words longer than 100 characters to avoid undetected embedded images and other shit.\n",
    "        text=[\" \".join([y.replace(\"\\n\",\" \").strip() for y in i.split(\" \") if len(y)<100]) for i in text] \n",
    "       # text=[\"\".join([y for y in i if len(y)<100]) for i in text]\n",
    "    \n",
    "    return pd.Series([Title,h1,h2,h3,text,linkName,linkUrl])\n",
    "        \n",
    "#df.file[2:6].apply(FileInfo)\n",
    "print(\"This takes about 4 minutes\")\n",
    "print_time()        \n",
    "df[[\"HTML_Title\",\"h1\",\"h2\",\"h3\",\"HTML_text\",\"linkName\",\"linkUrl\"]]=df.filepath.apply(FileInfo)\n",
    "print_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f31cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"filepath\"][DoubleIndex]#.apply(FileInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515fd2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unify & drop columns.\n",
    "\n",
    "df.loc[df[\"HTML_text\"].apply(lambda x: x is not None), \"HTML\"]=True\n",
    "df[\"HTML\"]=df[\"HTML\"].fillna(False)\n",
    "\n",
    "df.loc[df[\"HTML\"], \"text\"]=df.HTML_text\n",
    "df.loc[df[\"HTML\"], \"title\"]=df.HTML_Title\n",
    "df.loc[df[\"HTML\"], \"links\"]=df.linkUrl\n",
    "\n",
    "df.language=df.language.replace([\"en-US\",\"en-GB\",\"en-us\",\"en_US\",\"en_US\",\"English\"],\"en\")\n",
    "df.language=df.language.replace([\"nl\",\"nl-NL\",\"nederlands\",\"nl-nl\"],\"nl\")\n",
    "df.language=df.language.replace(\"\",None)\n",
    "\n",
    "#improve text...\n",
    "df.text=df.text.fillna(\"\").apply(lambda l: [s.replace(\"..\",\". \").replace(\". . \",\"\").lstrip(\". \") for s in l if isinstance(s,str)])\n",
    "\n",
    "df.drop(columns=[\"HTML_text\",\"HTML_Title\",\"linkUrl\",\"key\",\"parentItem\",\"h1\",\"h2\",\"h3\"],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf95e13a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33442ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "DoubleIndex=df[df.text.duplicated()].index\n",
    "#df=df.drop_duplicates(subset=\"text\")\n",
    "df.filepath[DoubleIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c99ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"2023-07-30_Uni_Files_NLP.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586e1b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(\"2023-07-30_Uni_Files_NLP.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b75fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Insert_title(text,Title):\n",
    "    if Title!=None and type(text)==list:\n",
    "        text=[Title.strip()]+text\n",
    "        return text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "\n",
    "df[\"text\"]=df[[\"text\",\"title\"]].apply(lambda x: Insert_title(*x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5994d5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b043d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Splitter(paragraphs, max_len=5000):\n",
    "    \n",
    "    sentences=[]\n",
    "    paragraphs_out=[]\n",
    "    if isinstance(paragraphs,list):\n",
    "        \n",
    "        for paragraph in paragraphs:\n",
    "            paragraph_out=[] \n",
    "       \n",
    "            from nltk.tokenize import sent_tokenize\n",
    "            \n",
    "\n",
    "            \n",
    "        \n",
    "            paragraph=paragraph.replace(\"\\n\",\". \").replace(\"!\",\" \").replace(\"?\",\" \").replace(\"..\",\". \").replace(\". . \",\"\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # fill in spaces after dot if followed by capital character\n",
    "            import re\n",
    "            regex = r\"(?<=[a-zA-Z])\\.(?=[A-Z][a-z])\"\n",
    "            subst = \". \"\n",
    "            paragraph = re.sub(regex, subst, paragraph, 0, re.MULTILINE)\n",
    "            paragraph=paragraph.replace(\"  \",\" \").replace(\" .\",\".\").lstrip(\". \")\n",
    "            \n",
    "            sentence=sent_tokenize(paragraph) # per paragraph\n",
    "            sentence=[s for s in sentence if len(s)>1]\n",
    "\n",
    "            counter = 0\n",
    "            P=\"\" # will be filled with sentences untill it has 5000 characters.\n",
    "            for s in sentence:\n",
    "                counter += len(s) # calculate the lenght if it would be added.\n",
    "\n",
    "                if counter > max_len: # does not add the sentence to the paragraph if too long\n",
    "                    \n",
    "                    paragraph_out.append(P)\n",
    "                    \n",
    "                    L=len(paragraph_out[-1])\n",
    "                    print(f\"{L} + {len(s)} = {counter} --> split {len(paragraph_out)} times.\")\n",
    "                \n",
    "                    counter=0\n",
    "                    P=\"\"\n",
    "                    \n",
    "                if counter < max_len:\n",
    "                    P=P.strip()+\" \"+ s.strip()\n",
    "            \n",
    "            paragraph_out.append(P)\n",
    "\n",
    "            sentences+=sentence\n",
    "            \n",
    "            paragraphs_out.extend(paragraph_out)\n",
    "                \n",
    "                \n",
    "    sentences_len=[len(i) for i in sentences]\n",
    "    paragraphs_len=[len(i) for i in paragraphs_out]\n",
    " \n",
    "        \n",
    "    return pd.Series([paragraphs_out,sentences,paragraphs_len,sentences_len])\n",
    "\n",
    "\n",
    "df[[\"paragraphs\",\"sentences\",\"paragraphs_len\",\"sentences_len\"]]=df[\"text\"].apply(Splitter, max_len=4500)\n",
    "df[\"paragraph_sum\"]=df.paragraphs_len.apply(lambda x: sum(x))\n",
    "\n",
    "#df[\"text\"].apply(Splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eaec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the lenght of paragraphs\n",
    "\n",
    "df[df.paragraphs_len.apply(lambda x: any(i > 5000 for i in x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7574bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# just checkin if the long paragraphs are necessary...\n",
    "df=df[df.title!='Research Posters - Faculty of Geosciences - Utrecht University']\n",
    "\n",
    "#df=df[df[\"paragraph_sum\"]<100000]\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', 255)\n",
    "\n",
    "df[df[\"paragraph_sum\"]>100000].filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a210ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"2023-07-30_Uni_Files_NLP_splitter.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d95145",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(\"2023-07-30_Uni_Files_NLP_splitter.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9827c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8b8ea9",
   "metadata": {},
   "source": [
    "# Explode --> by sentence or by paragraph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=df.explode(\"sentences\").drop_duplicates(subset=\"sentences\").reset_index(drop=True)\n",
    "\n",
    "df.paragraphs.explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e9e305",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.language.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e701bac",
   "metadata": {},
   "source": [
    "### I will filter in the analysis file. Therefore this code is not needed anymore...\n",
    "\n",
    "ChatGPT_Terms=\"ChatGPT, Chat-GPT, GPT3, GPT-3, GPT-x, GPT-4, GPT4,\\\n",
    "Transformer, OpenAI, AI, hallucination, Text generation, LLM, GPT, Chatbot, Models, generative, Intelligence, Model\"\n",
    "\n",
    "import MyLib.nlp as nlp\n",
    "\n",
    "df[\"AI_paragraphs\"]=df.paragraphs.dropna().apply(nlp.filter_paragraphs,by=ChatGPT_Terms).dropna()\n",
    "\n",
    "df[\"AI_Paragraphs_len\"]=df[\"AI_paragraphs\"].apply(lambda x: [len(i) for i in x])\n",
    "df[df.AI_Paragraphs_len.apply(lambda x: any(i > 4999 for i in x))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46d097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "column=\"sentences\"\n",
    "\n",
    "#df=df[df.file.apply(lambda x: x.endswith(\"html\"))]\n",
    "\n",
    "df=df.explode(column).reset_index(drop=True)\n",
    "df=df[df[column].apply(lambda x: type(x)==str)]\n",
    "df=df.drop_duplicates(column)\n",
    "df[\"LEN\"]=df[column].apply(lambda x: len(x.split(\" \")))\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae57fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b21dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df.sentences.apply(lambda x: len(x.replace('[^a-zA-Z]', ''))<10)][\"sentences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83074a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove tables of content\n",
    "df=df[df.sentences.apply(lambda x: x.count(\".\")<10)]\n",
    "\n",
    "#remove non-sense sentences by: if sentence is shorter than 10 characters without numbers.\n",
    "df=df[df.sentences.apply(lambda x: len(x.replace('[^a-zA-Z]', ''))>10)]\n",
    "\n",
    "df=df[df.sentences.apply(len)>15]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda1dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "PDF=df[df.HTML==False]\n",
    "\n",
    "# test what the pdf sentences with dots look like now.\n",
    "\n",
    "PDF[PDF.sentences.apply(lambda x: x.count(\".\")>8)].sentences.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ed35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check: remove short strings that dont resemble sentences.\n",
    "\n",
    "n=16 # ab 17\n",
    "c=df[df.sentences.apply(len)<n]\n",
    "print(\"LEN: \", len(c))      \n",
    "[print(i) for i in df[df.sentences.apply(len)<n].sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d717d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.sentences.apply(len)>4500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182cdd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2fef0",
   "metadata": {},
   "source": [
    "# RUN THE NLP PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a820e92e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "test=pd.DataFrame(df.head(5).to_dict())\n",
    "test[\"t\"]=test.url\n",
    "print(pd. __version__)\n",
    "\n",
    "test=nlp.NLP_Pipeline(test, text_column=column, target_language=\"en\",sentiment=False)\n",
    "test[\"source_language\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc08692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(20).sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cf2d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## takes approximately 120 minutes for translation (last time)\n",
    "df=nlp.NLP_Pipeline(df, text_column=column, target_language=\"en\",sentiment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707c434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5288790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"2023-07-27_ChatGPT_Sentences_NLP-Out.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415caa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "\n",
    "df2=df[['Uni','FileKey','url','date', 'Title','linkName', 'linkUrl', 'AI_paragraphs','text_clean', 'letters_count', 'word_count',\n",
    "       'language', 'source_language', 'pure_text', 'Lemmata', 'NoStopwords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dc5a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_json(\"2023-06-06_Zotero_AI_nlp_en2.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
