{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## My API key  & library_Id are stored in another file\n",
    "api_key,library_id = pd.read_json(\"Zotero_API_key.json\", typ='series')\n",
    "\n",
    "library_type=\"group\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyzotero import zotero\n",
    "zot = zotero.Zotero(library_id, library_type, api_key)\n",
    "\n",
    "ID_Universities='EWSYI3RS' # to access the files in the university subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff5a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in zot.collections_sub(ID_Universities):\n",
    "    name,key=i[\"data\"][\"name\"],i[\"data\"][\"key\"]\n",
    "    print(f\"name: {name} --> key: {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f16c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevantItemInfo(dictionary):    \n",
    "    if dictionary[\"itemType\"]=='attachment':\n",
    "        # if directory --> key leads to the correct folder\n",
    "        \n",
    "        relevant_keys=['key','url']\n",
    "        newDict={key: dictionary[key] for key in relevant_keys}\n",
    "        #print(dictionary)\n",
    "        newDict[\"note\"]=None\n",
    "       # print(newDict)\n",
    "        return newDict\n",
    "    \n",
    "    if dictionary[\"itemType\"]==\"note\":\n",
    "        relevant_keys=['key','note']\n",
    "        newDict={key: dictionary[key] for key in relevant_keys}\n",
    "        newDict[\"url\"]=None\n",
    "        return newDict\n",
    "\n",
    "    \n",
    "\n",
    "for i in zot.collection_items(key)[:2]:\n",
    "    dictionary=i[\"data\"]\n",
    "    print(dictionary, end=\"\\n\\n\")\n",
    "    \n",
    "    Info=relevantItemInfo(dictionary)\n",
    "    print(Info, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c774117",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictList=[]\n",
    "\n",
    "for i in zot.collections_sub(ID_Universities):\n",
    "    name,key=i[\"data\"][\"name\"],i[\"data\"][\"key\"]\n",
    "    print(f\"name: {name} --> key: {key}\")\n",
    "    \n",
    "    for i in zot.collection_items(key):\n",
    "        # only use data if there is an attachment.\n",
    "        if i[\"data\"][\"itemType\"]=='attachment':\n",
    "            Info=relevantItemInfo(i[\"data\"])\n",
    "            dictList+=[{\"Uni\": name,\"FileKey\":Info[\"key\"],\"url\":Info[\"url\"]}]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc36e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(dictList)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2234f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FileLinks(Filekey):\n",
    "   #print(Filekey)\n",
    "    import os\n",
    "    DIR=f\"C:\\\\Users\\\\mr\\\\Zotero\\\\storage\\\\{Filekey}\"\n",
    "    #print(DIR)\n",
    "    FileNames=os.listdir(DIR)\n",
    "    file=[i for i in FileNames if i.endswith(\".html\")]\n",
    "    return DIR+\"\\\\\"+FileNames[-1]\n",
    "\n",
    "\n",
    "\n",
    "df[\"file\"]=df.FileKey.apply(FileLinks)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96e1c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"2023-05-10_Uni_Files_raw.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f8a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(\"2023-05-10_Uni_Files_raw.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f98c63",
   "metadata": {},
   "source": [
    "# Add Content from HTMLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b8939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from htmldate import find_date\n",
    "#find_date(\"https://www.uu.nl/en/events/chat-gpt-discover-the-magic-of-ai\")\n",
    "\n",
    "def find_dateX(x):\n",
    "    try:\n",
    "        return find_date(x)\n",
    "    except:\n",
    "        print(x)\n",
    "\n",
    "\n",
    "df[\"date\"]=df.url.apply(find_dateX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb75526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MyLib.nlp as nlp\n",
    "import MyLib.HTML_prep as HTML_prep\n",
    "import MyLib.analysis as analysis \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949a4b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def FileInfo(FilePath):\n",
    "    Title,h1,h2,text,linkName,linkUrl=None,None,None,None,None,None\n",
    "    if FilePath.endswith(\".html\"):\n",
    "    \n",
    "        try:\n",
    "            #print(FilePath)\n",
    "\n",
    "            with open(FilePath,\"r\", encoding='utf-8') as f:\n",
    "                text= f.read()\n",
    "\n",
    "            from bs4 import BeautifulSoup\n",
    "\n",
    "            soup = BeautifulSoup(text, 'html.parser')\n",
    "            \n",
    "            Title=soup.title.text\n",
    "            \n",
    "            #soup=HTML_prep.removeIMGs(soup,IMG_filename=Title.replace(\" \",\"_\"))      \n",
    "\n",
    "\n",
    "            All_divs=[p.parent for p in soup(\"p\") if p.parent.name==\"div\"]\n",
    "            All_divs=(list(set(All_divs)))\n",
    "            All_divs=[HTML_prep.removeIMGs(div,no_img=True,IMG_filename=Title.replace(\" \",\"_\")) for div in All_divs]        \n",
    "\n",
    "            h1=[item for sublist in [[i.text for i in div(\"h1\")] for div in All_divs if div(\"h1\")!=None] for item in sublist]\n",
    "            h2=[item for sublist in [[i.text for i in div(\"h2\")] for div in All_divs if div(\"h2\")!=None] for item in sublist]\n",
    "            text=[item for sublist in [[i.text for i in div(\"p\")] for div in All_divs if div(\"p\")!=None] for item in sublist]\n",
    "            linkName=[item for sublist in [[i.text for i in div(\"a\")] for div in All_divs if div(\"a\")!=None] for item in sublist]\n",
    "            linkUrl=[item for sublist in [[i.get('href') for i in div(\"a\")] for div in All_divs if div(\"a\")!=None] for item in sublist]\n",
    "\n",
    "\n",
    "        except:\n",
    "            print(f\"error with: {FilePath}\",end=\". \")\n",
    "    return pd.Series([Title,h1,h2,text,linkName,linkUrl])\n",
    "        \n",
    "        \n",
    "\n",
    "#df.file[2:6].apply(FileInfo)\n",
    "df[[\"Title\",\"h1\",\"h2\",\"text\",\"linkName\",\"linkUrl\"]]=df.file.apply(FileInfo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c99ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"2023-05-10_Uni_Files_NLP.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b75fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Insert_title(text,Title):\n",
    "    if Title!=None and type(text)==list:\n",
    "        text=[Title]+text\n",
    "        return text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "\n",
    "df[\"text\"]=df[[\"text\",\"Title\"]].apply(lambda x: Insert_title(*x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eaa204",
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatGPT_Terms=\"ChatGPT, GPT3, GPT4, OpenAI, AI, LLM, A.I, GPT, Chatbot, technology, tool, app, Artificial Intelligence, Large Language Models\"\n",
    "ChatGPT_Terms=ChatGPT_Terms.split(\", \")\n",
    "ChatGPT_Terms=[i.lower() for i in ChatGPT_Terms]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0702aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPTparagraphs(p_list, by=ChatGPT_Terms):\n",
    "    if p_list!=None:\n",
    "        AI_Content=[]\n",
    "        for text in p_list:\n",
    "            AI_Content_Binary=nlp.WordlistFilter(text, by)\n",
    "            if AI_Content_Binary==True:\n",
    "                AI_Content.append(text)\n",
    "        return AI_Content\n",
    "            \n",
    "\n",
    "\n",
    "df[\"AI_paragraphs\"]=df.text.apply(GPTparagraphs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a562e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df[df.file.apply(lambda x: x.endswith(\"html\"))]\n",
    "df2.text=df2[\"AI_paragraphs\"].apply(lambda x:\" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495a14cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2.text.apply(lambda x: type(x))!=str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca323ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"LEN\"]=df2.text.apply(lambda x: len(x))\n",
    "df2=df2[df2.LEN<df2.LEN.max()] ###Cut out the very large list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea308524",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.LEN.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb10742",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=nlp.NLP_Pipeline(df2, sentiment=False, language=\"en\",translate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dc5a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_json(\"2023-05-10_Zotero_AI_nlp_en.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983e0050",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lists=df3.NoStopwords.to_list()\n",
    "Lists=[i for i in Lists if i!=None]\n",
    "Words=[item for sublist in Lists for item in sublist]\n",
    "\n",
    "from collections import Counter\n",
    "a_counter = Counter(Words)\n",
    "most_common = a_counter.most_common(10)\n",
    "\n",
    "most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bebd67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interactive, interact\n",
    "\n",
    "@interact(column=[\"NoStopwords\"],Mostcommon=(5,200))\n",
    "def make_wordcloud(column=\"NoStopwords\",Mostcommon=20,Save_as=\"\"):\n",
    "    hashtags=[i.lower() for s in df3[column].dropna() for i in s] # hashtags OR # no stopwords\n",
    "    analysis.make_wordcloud(hashtags,filename=Save_as,Mostcommon=Mostcommon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dd0982",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_excel(\"2023-05-08_ZoteroHTMLs.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
