{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c80aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import MyLib.nlp as nlp\n",
    "import MyLib.HTML_prep as HTML_prep\n",
    "import MyLib.PDF_prep as PDF_prep\n",
    "import MyLib.analysis as analysis \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from numpy import nan\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "## My API key  & library_Id are stored in another file\n",
    "api_key,library_id = pd.read_json(\"Zotero_API_key.json\", typ='series')\n",
    "\n",
    "library_type=\"group\"\n",
    "\n",
    "def print_time():\n",
    "    from datetime import datetime\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Current Time =\", current_time)\n",
    "\n",
    "def d(l):\n",
    "    pd.set_option('display.max_colwidth', l)\n",
    "\n",
    "\n",
    "\n",
    "from pyzotero import zotero\n",
    "zot = zotero.Zotero(library_id, library_type, api_key)\n",
    "\n",
    "ID_Universities='EWSYI3RS' # to access the files in the university subfolder.\n",
    "ID_SURF='WPXRQVIU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c774117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Radboud University --> key: 5JQEUPHJ\n",
      "name: Leiden University --> key: P483KEM8\n",
      "name: OpenUniversiteitNederland --> key: X4HX8ZQE\n",
      "name: Wageningen University --> key: QMTMEV7G\n",
      "name: University of Twente --> key: QFGTICY2\n",
      "name: Utrecht University --> key: 9A2UJIVA\n",
      "name: Rotterdam University --> key: Z4EJ3VVG\n",
      "name: University of Groningen --> key: P63XN58M\n",
      "name: Vrije Universiteit Amsterdam --> key: AZJ9ZEBZ\n",
      "name: Tilburg University --> key: XF9572P7\n",
      "name: Eindhoven University of Technoloy --> key: FKNPJ5UD\n",
      "name: Maastricht Univerisity --> key: QW34VSNN\n",
      "name: Delft University --> key: FZDYP465\n",
      "name: University of Amsterdam --> key: 92FECCMX\n",
      "name: surf --> key: WPXRQVIU\n",
      "1236\n"
     ]
    }
   ],
   "source": [
    "helpDict={}\n",
    "\n",
    "for i in zot.collections_sub(ID_Universities):\n",
    "    name,key=i[\"data\"][\"name\"],i[\"data\"][\"key\"]\n",
    "    print(f\"name: {name} --> key: {key}\")\n",
    "    collection_items=zot.collection_items(key)\n",
    "    helpDict.update({k[\"key\"]:k[\"data\"]|{\"Uni\":name} for k in collection_items})\n",
    "    \n",
    "#Add Surf\n",
    "name=\"surf\"\n",
    "key=ID_SURF\n",
    "print(f\"name: {name} --> key: {key}\")\n",
    "collection_items=zot.collection_items(key)\n",
    "helpDict.update({k[\"key\"]:k[\"data\"]|{\"Uni\":name} for k in collection_items})\n",
    "df=pd.DataFrame(helpDict).T\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b7f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Claimed_Parents=df[\"parentItem\"].dropna().to_list()\n",
    "Real_Parents=df.index.to_list()\n",
    "WithoutParents=list(set(Claimed_Parents)-set(Real_Parents))\n",
    "WithoutParents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2855183",
   "metadata": {},
   "outputs": [],
   "source": [
    "Parents_columns=['title', 'date','itemType','language']\n",
    "WithParents_columns=[\"key\",'parentItem','url','filename',\"Uni\"]\n",
    "\n",
    "WithParents=df.dropna(subset=\"parentItem\")[WithParents_columns]\n",
    "Parents=df[df.itemType.apply(lambda x: x!=\"attachment\")][Parents_columns]\n",
    "\n",
    "df=WithParents.merge(Parents, left_on=\"parentItem\",right_index=True, how=\"right\")\n",
    "df.head(5)\n",
    "\n",
    "#df2[df2.filename.isna()].itemType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec46b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Uni.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df35375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct item type of uTwente Thesies\n",
    "df.loc[df.url.apply(lambda x: \"essay.utwente.nl\" in str(x)),\"itemType\"]=\"thesis\"\n",
    "print(df.itemType.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d0d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove not useful items\n",
    "DropTypeList=[\"journalArticle\",\"conferencePaper\",\"thesis\",\"book\",\"bookSection\",\"note\",\"film\",\"dataset\"]\n",
    "\n",
    "df=df[~df[\"itemType\"].isin(DropTypeList)]\n",
    "\n",
    "print(df.itemType.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012ea62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileName(key,filename):\n",
    "    DIR=f\"C:\\\\Users\\\\mr\\\\Zotero\\\\storage\\\\{key}\\\\\"\n",
    "    filename=DIR+str(filename)\n",
    "    return filename\n",
    "\n",
    "df[\"filepath\"]=df.apply(lambda x: fileName(x.key, x.filename), axis=1).drop_duplicates()\n",
    "\n",
    "print(df.filepath[0],print(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ae84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"2023-09-11_Uni_Files_raw.json\")\n",
    "# CHeck for duplicates / errors in Zotero: \n",
    "df[df.index.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99b692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(\"2023-09-11_Uni_Files_raw.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb7a3f",
   "metadata": {},
   "source": [
    "# Add Content from PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dd5ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is used to improve single pdf imports\n",
    "\n",
    "##paragraph=PDF_prep.extract_text_with_pyPDF(df.filepath[4914],MaxPages=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f8a5e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PDFs=df.loc[df.filepath.apply(lambda x: x.split(\".\")[-1]==\"pdf\"),[\"filepath\",\"date\"]]    \n",
    "NumberOfPDFs=len(PDFs)    \n",
    "\n",
    "print(f\"the dataset containts {NumberOfPDFs} PDF-files.\")\n",
    "\n",
    "df[[\"text\",\"links\",\"mod_date\"]]=PDFs.filepath.progress_apply(PDF_prep.extract_text_with_pyPDF,MaxPages=150)\n",
    "\n",
    "# Use filemod as date.\n",
    "df.loc[~df.mod_date.isna(),[\"date\"]]=df.mod_date\n",
    "df.drop(columns='mod_date',inplace=True)\n",
    "\n",
    "#problemfile.filepath.apply(extract_text_with_pyPDF,MaxPages=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f98c63",
   "metadata": {},
   "source": [
    "# Add Content from HTMLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8dee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_time() \n",
    "\n",
    "print(\"This takes about: 3 minutes.\")\n",
    "\n",
    "df[\"date\"]=df.date.apply(pd.to_datetime,errors='coerce')\n",
    "df[\"date\"]=df.progress_apply(lambda x: HTML_prep.find_date(x.url, x.date), axis=1)\n",
    "print_time()\n",
    "\n",
    "df.date.apply(lambda x: type(x)==pd.Timestamp).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4f2046",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"2023-09-12_Uni_Files_raw_date.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d44a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(\"2023-09-12_Uni_Files_raw_date.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb721697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "sent_tokenize(\"Heute gehe ich heim, Dr. Klaas nla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bc0b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###USE NLTK or Spacy for a better tokanizer\n",
    "\n",
    "def extract_HTML(FilePath):\n",
    "    \n",
    "    Title,h1,h2,h3,text,linkName,linkUrl=None,None,None,None,None,None,None\n",
    "    \n",
    "    if FilePath.endswith(\".pdf\"):\n",
    "        return pd.Series([Title,h1,h2,h3,text,linkName,linkUrl])\n",
    "    \n",
    "    text=HTML_prep.open_html_file(FilePath)\n",
    "    \n",
    "    if text:\n",
    "        All_divs,Title=HTML_prep.return_content_soup(text)\n",
    "        h1,h2,h3,linkName,LinkUrl=HTML_prep.get_HTML_elements_from_soup(All_divs)\n",
    "    \n",
    "        try:\n",
    "            #text=get_text_from_soup_simple_split(All_divs) \n",
    "            text=HTML_prep.get_text_from_soup_with_nltk(All_divs)\n",
    "        except:\n",
    "            print(f\"error with: {FilePath} - read HTML only\", end=\". \")\n",
    "            text=HTML_prep.get_text_from_html(text)\n",
    "\n",
    "    return pd.Series([Title,h1,h2,h3,text,linkName,linkUrl])\n",
    "\n",
    "\n",
    "#df.file[2:6].apply(FileInfo)\n",
    "print(\"This takes about 3 minutes\")\n",
    "print_time()        \n",
    "df[[\"HTML_Title\",\"h1\",\"h2\",\"h3\",\"HTML_text\",\"linkName\",\"linkUrl\"]]=df.filepath.progress_apply(extract_HTML)\n",
    "print_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57652a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"2023-09-12_Uni_Files_NLP.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1931d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(\"2023-09-12_Uni_Files_NLP.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64270a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"HTML_text\"][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8fe43a",
   "metadata": {},
   "source": [
    "# Finalize the text before running the NLP stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515fd2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unify & drop columns.\n",
    "\n",
    "df.loc[df[\"HTML_text\"].apply(lambda x: x is not None), \"HTML\"]=True\n",
    "df[\"HTML\"]=df[\"HTML\"].fillna(False)\n",
    "\n",
    "df.loc[df[\"HTML\"], \"text\"]=df.HTML_text\n",
    "df.loc[df[\"HTML\"], \"title\"]=df.HTML_Title\n",
    "df.loc[df[\"HTML\"], \"links\"]=df.linkUrl\n",
    "\n",
    "df.language=df.language.replace([\"en-US\",\"en-GB\",\"en-us\",\"en_US\",\"en_US\",\"English\"],\"en\")\n",
    "df.language=df.language.replace([\"nl\",\"nl-NL\",\"nederlands\",\"nl-nl\"],\"nl\")\n",
    "df.language=df.language.replace(\"\",None)\n",
    "\n",
    "df.drop_duplicates(subset=\"text\",inplace=True)\n",
    "df.dropna(subset=\"text\",inplace=True)\n",
    "\n",
    "#df.drop(columns=[\"HTML_text\",\"HTML_Title\",\"linkUrl\",\"parentItem\",\"h1\",\"h2\",\"h3\"],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b75fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Insert_title(text,Title):\n",
    "    if Title!=None and type(text)==list:\n",
    "        text=[Title.strip()]+text\n",
    "        return text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "\n",
    "# I dont run this #    \n",
    "#df[\"text\"]=df[[\"text\",\"title\"]].apply(lambda x: Insert_title(*x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5371e642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1de67f16",
   "metadata": {},
   "source": [
    "# Edit & split text into paragraphs & sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff5f239",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"text\"].apply(lambda x: isinstance(x,list)!=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b043d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_long_paragraphs(paragraphs,\n",
    "                          max_char_in_paragraph=5000,\n",
    "                          min_char_in_paragraph=15,\n",
    "                          max_char_in_sentence=5000,\n",
    "                          min_char_in_sentence=4):\n",
    "    removed_sentences=[]\n",
    "    removed_paragraphs=[]\n",
    "    sentences_out=[]\n",
    "    paragraphs_out=[]\n",
    "    \n",
    "    if isinstance(paragraphs,list):\n",
    "        \n",
    "        for paragraph in paragraphs:\n",
    "            paragraph_out=[]      \n",
    "            \n",
    "            from nltk.tokenize import sent_tokenize  \n",
    "            \n",
    "            #e NLTK sentencer cannot handle A.I. \n",
    "            paragraph=paragraph.replace(\"A.I.\",\"AI\").replace(\"e.g.\",\"eg\")\n",
    "            \n",
    "            sentences=sent_tokenize(paragraph) # per paragraph\n",
    "            \n",
    "            counter = 0\n",
    "            P=\"\" # will be filled with sentences untill it has 5000 characters.\n",
    "            \n",
    "            for s in sentences:\n",
    "                Only_string_len=len(s.replace(\"[^a-zA-Z]\",\"\"))\n",
    "                if (Only_string_len<=min_char_in_sentence)|(Only_string_len>max_char_in_sentence):\n",
    "                    removed_sentences.append(s)\n",
    "                else:\n",
    "                    sentences_out.append(s)\n",
    "\n",
    "                counter += len(s) # calculate the lenght if it would be added.\n",
    "\n",
    "                if counter > max_char_in_paragraph: # does not add the sentence to the paragraph if too long\n",
    "                    \n",
    "                    paragraph_out.append(P)\n",
    "                    \n",
    "                    L=len(paragraph_out[-1])\n",
    "                    \n",
    "                    print(f\"{L} + {len(s)} = {counter} --> split {len(paragraph_out)} times.\")\n",
    "                    \n",
    "                    counter=0\n",
    "                    P=\"\"\n",
    "                    \n",
    "                if counter < max_char_in_paragraph:\n",
    "                    P=P.strip()+\" \"+ s.strip()\n",
    "                    \n",
    "            if len(P)>=min_char_in_paragraph:\n",
    "                # minipal lenght for a paragraph\n",
    "                paragraph_out.append(P.strip())\n",
    "                paragraphs_out.extend(paragraph_out)\n",
    "                \n",
    "            if len(P)<=min_char_in_paragraph:\n",
    "                removed_paragraphs.append(P)\n",
    "    \n",
    "    if False: ## Do I want this info?\n",
    "    \n",
    "        if len(removed_sentences)>0:\n",
    "            print(\"removed sentences:\")\n",
    "            print(removed_sentences)\n",
    "        if len(removed_paragraphs)>0:\n",
    "            print(\"removed paragraphs:\")\n",
    "            print(removed_paragraphs)  \n",
    "        \n",
    "        \n",
    "    sentences_len=[len(i) for i in sentences_out]\n",
    "    \n",
    "    paragraphs_len=[len(i) for i in paragraphs_out]\n",
    " \n",
    "        \n",
    "    return pd.Series([paragraphs_out,sentences_out,paragraphs_len,sentences_len,removed_paragraphs])\n",
    "\n",
    "\n",
    "df[[\"paragraphs\",\"sentences\",\"paragraphs_len\",\"sentences_len\",\"removed_paragraphs\"]]=df[\"text\"].apply(split_long_paragraphs, max_char_in_paragraph=4300)\n",
    "#df[\"paragraph_sum\"]=df.paragraphs_len.apply(lambda x: sum(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ce6e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.removed_paragraphs.explode().dropna().to_list()[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f948e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what sentences are almost as long as the max setting\n",
    "\n",
    "sss=df[df.sentences.apply(lambda x: any([len(i)>2900 for i in x]))][\"sentences\"]\n",
    "for s in sss:\n",
    "    for e,i in enumerate(s):\n",
    "        if len(i)>800:\n",
    "            print(e,i,len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e674324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.explode(\"sentences_len\")\n",
    "\n",
    "x[x.sentences_len<1000][\"sentences_len\"].plot.hist(bins=40)\n",
    "#x[\"sentences_len\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10813eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.explode(\"paragraphs_len\")\n",
    "\n",
    "x[x.paragraphs_len<5000][\"paragraphs_len\"].plot.hist(bins=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f004ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "d(20)\n",
    "df[[\"paragraphs\",\"sentences\",\"paragraphs_len\",\"sentences_len\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eaec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the lenght of paragraphs\n",
    "d(5000)\n",
    "\n",
    "# manually remove this -- its too long.\n",
    "df=df[df.title!='Research Posters - Faculty of Geosciences - Utrecht University']\n",
    "\n",
    "\n",
    "df[df.sentences_len.apply(lambda x: any(i > 4000 for i in x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e442cd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"paragraphs_len\"].apply(lambda x: any([p<10 for p in x]))][\"paragraphs_len\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d717d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is one column longer than what google translates.\n",
    "df[df[column].apply(len)>4900]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2fef0",
   "metadata": {},
   "source": [
    "# RUN THE NLP PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c137282",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=df.head(50).copy()\n",
    "\n",
    "print(\"TEST RUN\")\n",
    "\n",
    "source_column=\"text\" #or \"paragraphs\"\n",
    "translated_column=\"text\"#_translated\"\n",
    "test=test.explode(source_column)\n",
    "\n",
    "test[[\"text_translated\",\"source_language\"]]=test.apply(lambda x: nlp.GoogleTrans(x[source_column],x[\"language\"]), axis=1)\n",
    "\n",
    "test=nlp.NLP_Pipeline(test, text_column=translated_column, sentiment=False, metaphors=False)\n",
    "\n",
    "d(200)\n",
    "test[\"NoStopwords\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57d2097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate\n",
    "\n",
    "df=df.explode(\"paragraphs\")\n",
    "\n",
    "df[[\"text_translated\",\"source_language\"]]=df.progress_apply(lambda x: nlp.GoogleTrans(x[\"paragraphs\"],x[\"language\"]), axis=1)\n",
    "\n",
    "df['P_counter'] = df.groupby('key').cumcount()\n",
    "df[\"key_P\"]=df.apply(lambda x: str(x.key)+\"_\"+str(x.P_counter), axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9daf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns=[\"key\",\"key_P\",\"url\",\"filename\",\"Uni\",'itemType',\"date\",\\\n",
    "                  \"language\",\"filepath\",\"links\",\"text_translated\",\"source_language\"]\n",
    "df=df[relevant_columns]\n",
    "df.rename(columns={\"text_translated\":\"text\"},inplace=True\n",
    "          \n",
    "df.reset_index(inplace=True)\n",
    "df.to_json(\"2023-09-12_ChatGPT_translated.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f5c929f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>key_P</th>\n",
       "      <th>url</th>\n",
       "      <th>filename</th>\n",
       "      <th>Uni</th>\n",
       "      <th>itemType</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>filepath</th>\n",
       "      <th>links</th>\n",
       "      <th>text</th>\n",
       "      <th>source_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KYXS34U3</td>\n",
       "      <td>KYXS34U3_0</td>\n",
       "      <td>https://www.ru.nl/en/cls/clst</td>\n",
       "      <td>clst.html</td>\n",
       "      <td>Radboud University</td>\n",
       "      <td>webpage</td>\n",
       "      <td>2023-06-28</td>\n",
       "      <td>en</td>\n",
       "      <td>C:\\Users\\mr\\Zotero\\storage\\KYXS34U3\\clst.html</td>\n",
       "      <td>None</td>\n",
       "      <td>Centre for Language and Speech Technology We a...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KYXS34U3</td>\n",
       "      <td>KYXS34U3_1</td>\n",
       "      <td>https://www.ru.nl/en/cls/clst</td>\n",
       "      <td>clst.html</td>\n",
       "      <td>Radboud University</td>\n",
       "      <td>webpage</td>\n",
       "      <td>2023-06-28</td>\n",
       "      <td>en</td>\n",
       "      <td>C:\\Users\\mr\\Zotero\\storage\\KYXS34U3\\clst.html</td>\n",
       "      <td>None</td>\n",
       "      <td>The amount of information available in our dig...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KYXS34U3</td>\n",
       "      <td>KYXS34U3_2</td>\n",
       "      <td>https://www.ru.nl/en/cls/clst</td>\n",
       "      <td>clst.html</td>\n",
       "      <td>Radboud University</td>\n",
       "      <td>webpage</td>\n",
       "      <td>2023-06-28</td>\n",
       "      <td>en</td>\n",
       "      <td>C:\\Users\\mr\\Zotero\\storage\\KYXS34U3\\clst.html</td>\n",
       "      <td>None</td>\n",
       "      <td>Playfully practicing speaking and reading 28 J...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KYXS34U3</td>\n",
       "      <td>KYXS34U3_3</td>\n",
       "      <td>https://www.ru.nl/en/cls/clst</td>\n",
       "      <td>clst.html</td>\n",
       "      <td>Radboud University</td>\n",
       "      <td>webpage</td>\n",
       "      <td>2023-06-28</td>\n",
       "      <td>en</td>\n",
       "      <td>C:\\Users\\mr\\Zotero\\storage\\KYXS34U3\\clst.html</td>\n",
       "      <td>None</td>\n",
       "      <td>Learning to read better with software that lis...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KYXS34U3</td>\n",
       "      <td>KYXS34U3_4</td>\n",
       "      <td>https://www.ru.nl/en/cls/clst</td>\n",
       "      <td>clst.html</td>\n",
       "      <td>Radboud University</td>\n",
       "      <td>webpage</td>\n",
       "      <td>2023-06-28</td>\n",
       "      <td>en</td>\n",
       "      <td>C:\\Users\\mr\\Zotero\\storage\\KYXS34U3\\clst.html</td>\n",
       "      <td>None</td>\n",
       "      <td>HoMed (Homo Medicinalis) Research HoMed (Homo ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        key       key_P                            url   filename  \\\n",
       "0  KYXS34U3  KYXS34U3_0  https://www.ru.nl/en/cls/clst  clst.html   \n",
       "1  KYXS34U3  KYXS34U3_1  https://www.ru.nl/en/cls/clst  clst.html   \n",
       "2  KYXS34U3  KYXS34U3_2  https://www.ru.nl/en/cls/clst  clst.html   \n",
       "3  KYXS34U3  KYXS34U3_3  https://www.ru.nl/en/cls/clst  clst.html   \n",
       "4  KYXS34U3  KYXS34U3_4  https://www.ru.nl/en/cls/clst  clst.html   \n",
       "\n",
       "                  Uni itemType       date language  \\\n",
       "0  Radboud University  webpage 2023-06-28       en   \n",
       "1  Radboud University  webpage 2023-06-28       en   \n",
       "2  Radboud University  webpage 2023-06-28       en   \n",
       "3  Radboud University  webpage 2023-06-28       en   \n",
       "4  Radboud University  webpage 2023-06-28       en   \n",
       "\n",
       "                                        filepath links  \\\n",
       "0  C:\\Users\\mr\\Zotero\\storage\\KYXS34U3\\clst.html  None   \n",
       "1  C:\\Users\\mr\\Zotero\\storage\\KYXS34U3\\clst.html  None   \n",
       "2  C:\\Users\\mr\\Zotero\\storage\\KYXS34U3\\clst.html  None   \n",
       "3  C:\\Users\\mr\\Zotero\\storage\\KYXS34U3\\clst.html  None   \n",
       "4  C:\\Users\\mr\\Zotero\\storage\\KYXS34U3\\clst.html  None   \n",
       "\n",
       "                                                text source_language  \n",
       "0  Centre for Language and Speech Technology We a...              en  \n",
       "1  The amount of information available in our dig...              en  \n",
       "2  Playfully practicing speaking and reading 28 J...              en  \n",
       "3  Learning to read better with software that lis...              en  \n",
       "4  HoMed (Homo Medicinalis) Research HoMed (Homo ...              en  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_json(\"2023-09-12_ChatGPT_translated.json\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8775ef6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time = 00:18:56\n",
      "len:  4897\n",
      "splitting to sentences.\n",
      "len:  32964\n",
      "Token & Lemmatizing & stopword removal & modal_word.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 32964/32964 [50:23<00:00, 10.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time = 01:09:48\n"
     ]
    }
   ],
   "source": [
    "relevant_columns=[\"key\",\"key_P\",\"url\",\"filename\",\"Uni\",'itemType',\"date\",\\\n",
    "                  \"language\",\"filepath\",\"links\",\"text\",\"source_language\"]\n",
    "\n",
    "df=df[relevant_columns]\n",
    "df=nlp.NLP_Pipeline(df, text_column=\"text\", sentiment=False, metaphors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "754cd7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: url, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "d(5)\n",
    "\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76d1cb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwording done. Next: sentiment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 32963/32963 [41:07<00:00, 13.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=model, max_length=512, truncation=True)\n",
    "\n",
    "print(\"Stopwording done. Next: sentiment.\")                              \n",
    "df[\"sentiment\"] = df.progress_apply(nlp.roberta_sentiment,column=\"sentences\", axis=1,pipe=sentiment_pipe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39fd7969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now - metaphors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 32963/32963 [41:27<00:00, 13.25it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Now - metaphors.\")  \n",
    "\n",
    "stop_words= list(set(stopwords.words('english')))\n",
    "\n",
    "\n",
    "metaphor_pipe = pipeline(\"token-classification\", model=\"CreativeLang/metaphor_detection_roberta_seq\")\n",
    "\n",
    "df[\"metaphors\"] = df.progress_apply(nlp.classify_metaphors, axis=1,column=\"sentences\",stop_words=stop_words,pipe=metaphor_pipe)\n",
    "df[\"metaphors_n\"] = df.metaphors.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a0842fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.reset_index(inplace=True)\n",
    "df.to_json(\"2023-09-14_ChatGPT_NLP_met_sent.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f89e6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
